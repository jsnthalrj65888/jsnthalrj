# 功能特性详解

## 🎯 核心功能

### 1. 浏览器自动化

**技术实现**: Playwright

**功能特点**:
- ✅ 完全模拟真实浏览器行为
- ✅ 支持JavaScript动态内容加载
- ✅ 自动滚动页面加载懒加载图片
- ✅ 随机User-Agent轮换
- ✅ Cookie和Session管理
- ✅ 支持无头模式和可见模式

**使用示例**:
```bash
# 无头模式（默认）
python main.py

# 显示浏览器窗口（调试）
python main.py --no-headless
```

### 2. 代理管理系统

**类**: `ProxyManager`

**功能特点**:
- ✅ 支持HTTP/HTTPS/SOCKS5代理
- ✅ 代理池自动轮换
- ✅ 失败代理自动屏蔽（5分钟后恢复）
- ✅ 代理可用性测试
- ✅ 支持认证代理（用户名/密码）

**代理格式**:
```
http://IP:端口
http://用户名:密码@IP:端口
https://IP:端口
socks5://IP:端口
```

**使用方式**:

方式1 - 配置文件:
```bash
# proxies.txt
http://127.0.0.1:7890
http://user:pass@proxy.example.com:8080
```

方式2 - config.py:
```python
PROXY_LIST = [
    'http://127.0.0.1:7890',
    'http://proxy.example.com:8080',
]
```

### 3. 递归爬取引擎

**功能特点**:
- ✅ 深度优先遍历
- ✅ URL自动去重
- ✅ 深度限制控制
- ✅ 页面数量限制
- ✅ 智能链接过滤（同域名）
- ✅ robots.txt遵守

**控制参数**:
```bash
--depth N        # 最大深度（默认3）
--max-pages N    # 最大页面数（默认50）
```

**工作流程**:
1. 从起始URL开始
2. 提取页面中的所有链接
3. 过滤同域名链接
4. 添加到待爬队列
5. 递归爬取（直到达到限制）

### 4. 图片下载系统

**功能特点**:
- ✅ 多线程并发下载
- ✅ 图片格式验证（Pillow）
- ✅ 文件大小过滤
- ✅ 断点续传（跳过已下载）
- ✅ MD5去重命名
- ✅ 自动重试机制
- ✅ 进度条显示（tqdm）

**支持格式**:
- JPG/JPEG
- PNG
- GIF
- WebP
- BMP

**下载流程**:
1. 提取图片URL
2. 检查是否已下载
3. 验证图片大小
4. 下载图片内容
5. 验证图片格式
6. 保存到本地

**配置项**:
```python
MIN_IMAGE_SIZE = 10240  # 最小10KB
MAX_WORKERS = 5         # 并发线程数
MAX_RETRIES = 3         # 重试次数
```

### 5. 日志系统

**功能特点**:
- ✅ 多级别日志（DEBUG/INFO/WARNING/ERROR）
- ✅ 文件日志（详细）
- ✅ 控制台日志（重要信息）
- ✅ 时间戳记录
- ✅ 中文支持

**日志内容**:
- 页面访问记录
- 图片发现数量
- 下载成功/失败
- 错误和警告
- 代理使用情况
- 统计信息

**日志位置**:
```
logs/crawler_YYYYMMDD_HHMMSS.log
```

### 6. 反爬虫策略

**实现机制**:

1. **随机延迟**
   ```python
   MIN_DELAY = 1  # 最小1秒
   MAX_DELAY = 3  # 最大3秒
   ```

2. **User-Agent轮换**
   - Chrome (Windows/Mac/Linux)
   - Firefox
   - Safari

3. **代理IP轮换**
   - 每次请求随机选择
   - 失败自动切换

4. **请求头伪装**
   - Referer设置
   - Accept headers
   - Cookie管理

5. **Robots.txt遵守**
   ```python
   RESPECT_ROBOTS_TXT = True
   ```

## 🔧 高级功能

### 1. 断点续传

**实现**: 文件名MD5哈希

```python
# 计算URL的MD5作为文件名
filename = hashlib.md5(url.encode()).hexdigest() + ext
```

**效果**:
- 重新运行不会重复下载
- 节省时间和带宽
- 支持中断恢复

**控制**:
```bash
# 启用（默认）
python main.py

# 禁用（重新下载）
python main.py --no-skip-existing
```

### 2. 进度显示

**库**: tqdm

**显示内容**:
- 当前页面名称
- 下载进度条
- 下载速度
- 剩余时间
- 已完成/总数

**示例输出**:
```
下载图片 [index]: 100%|████████| 25/25 [00:10<00:00,  2.45it/s]
下载图片 [gallery]: 75%|██████  | 15/20 [00:05<00:02,  1.89it/s]
```

### 3. 统计报告

**统计项**:
- 总耗时
- 页面爬取数
- 图片发现数
- 图片下载成功/失败/跳过
- 下载成功率
- 代理使用情况

**示例输出**:
```
============================================================
爬取完成！统计信息:
============================================================
总耗时: 123.45 秒
页面爬取数: 25
图片发现数: 250
图片下载成功: 230
图片下载失败: 5
图片跳过: 15
下载成功率: 92.00%
代理统计: 总数=5, 可用=4, 失败=1
============================================================
```

### 4. 命令行界面

**框架**: argparse

**支持参数**:
- URL配置
- 深度和页面数
- 输出目录
- 并发设置
- 代理设置
- 延迟配置
- 浏览器模式

**帮助信息**:
```bash
python main.py --help
```

### 5. 环境变量支持

**库**: python-dotenv

**配置文件**: `.env`

**支持项**:
- 所有config.py中的配置
- 敏感信息（代理密码等）
- 运行环境区分

**优先级**:
1. 命令行参数（最高）
2. 环境变量
3. config.py默认值

## 📊 性能优化

### 1. 并发下载

**技术**: ThreadPoolExecutor

**优势**:
- 充分利用CPU
- 提高下载速度
- 自动任务分配

**配置**:
```bash
--workers N  # 并发数（默认5）
```

**建议值**:
- 小规模: 3-5
- 中等规模: 5-10
- 大规模: 10-20

### 2. 懒加载处理

**实现**:
```python
# 滚动页面触发懒加载
page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
time.sleep(1)
```

**效果**:
- 加载所有图片
- 支持无限滚动
- 完整内容获取

### 3. 图片预验证

**检查项**:
1. 文件大小（MIN_IMAGE_SIZE）
2. 图片格式（Pillow验证）
3. 是否已下载

**好处**:
- 避免下载无效图片
- 节省带宽
- 减少存储空间

## 🛡️ 安全特性

### 1. 异常处理

**覆盖范围**:
- 网络错误
- 超时错误
- 解析错误
- 文件IO错误
- 代理错误

**处理方式**:
- 记录日志
- 自动重试
- 优雅降级
- 统计记录

### 2. 资源清理

**实现**:
- 浏览器自动关闭
- 文件句柄正确关闭
- 线程池安全退出
- 临时文件清理

### 3. 中断处理

**支持**:
- Ctrl+C优雅退出
- 保存已下载状态
- 显示统计信息
- 清理资源

## 🎨 可扩展性

### 1. 模块化设计

**模块**:
- `crawler.py` - 爬虫核心
- `proxy_manager.py` - 代理管理
- `config.py` - 配置管理
- `logger_config.py` - 日志配置

**优势**:
- 易于维护
- 便于测试
- 支持扩展

### 2. 配置灵活

**多种方式**:
- 命令行参数
- 环境变量
- 配置文件
- 代码修改

### 3. 易于定制

**常见定制**:
- 自定义图片过滤
- 自定义链接过滤
- 自定义存储格式
- 自定义代理策略

## 📝 代码质量

**特点**:
- ✅ 类型提示（Type Hints）
- ✅ 文档字符串（Docstrings）
- ✅ 错误处理完善
- ✅ 日志记录详细
- ✅ 代码结构清晰
- ✅ 遵循PEP8规范

## 🔍 调试功能

### 1. 可见浏览器模式

```bash
python main.py --no-headless
```

### 2. 详细日志

```bash
# 查看日志
tail -f logs/crawler_*.log

# 只看错误
grep ERROR logs/crawler_*.log
```

### 3. 小规模测试

```bash
python main.py --depth 0 --max-pages 1
```

## 🚀 未来扩展方向

可能的增强功能：
- [ ] 数据库存储（SQLite/MySQL）
- [ ] 图片去重（感知哈希）
- [ ] 分布式爬取
- [ ] Web管理界面
- [ ] API接口
- [ ] 图片压缩
- [ ] 缩略图生成
- [ ] 元数据提取
- [ ] 云存储支持
- [ ] Docker部署
